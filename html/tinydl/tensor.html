<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>tinydl.tensor API documentation</title>
<meta name="description" content="Tensor Class." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>tinydl.tensor</code></h1>
</header>
<section id="section-intro">
<p>Tensor Class.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># Credits to https://github.com/kartik4949/deepops


&#34;&#34;&#34;Tensor Class.&#34;&#34;&#34;
import functools
import operator

import numpy as np

# PyCUDA initialization

import pycuda.driver as cuda
import pycuda.autoinit
from pycuda.compiler import SourceModule
from enum import Enum


class TensorState(Enum):
    &#34;&#34;&#34;Tensor State which tells the current state of Tensor i.e
    Detach, Device, Host.
    &#34;&#34;&#34;

    DEVICE = &#34;DEVICE&#34;
    HOST = &#34;HOST&#34;
    DETACH = &#34;DETACH&#34;


__all__ = [&#34;add&#34;, &#34;transpose&#34;, &#34;mul&#34;]


class Kernel:
    def addition_kernel(self):
        add = SourceModule(
            &#34;&#34;&#34;
          __global__ void device_vec_add(float * __restrict__ d_c, const float * __restrict__ d_a, const float * __restrict__ d_b, const int N)
          {
            const int tid = threadIdx.x + blockIdx.x * blockDim.x;
            if (tid &gt;= N) return;
            d_c[tid] = d_a[tid] + d_b[tid];
          }
          &#34;&#34;&#34;
        )
        return add

    def arithmetic_kernel(self, operator):
        operation = SourceModule(
            &#34;&#34;&#34;
          __global__ void device_arithmetic(float * __restrict__ d_c, const float * __restrict__ d_a, const float * __restrict__ d_b, const int N)
          {
            const int tid = threadIdx.x + blockIdx.x * blockDim.x;
            if (tid &gt;= N) return;
            d_c[tid] = d_a[tid] %s d_b[tid];
          }
          &#34;&#34;&#34;
            % operator
        )
        return operation

    def multiply_kernel(self):
        mul = SourceModule(
            &#34;&#34;&#34;
        __global__ void multiply_them(float *dest, float *a, float *b)
        {
          const int i = threadIdx.x;
          dest[i] = a[i] * b[i];
        }
        &#34;&#34;&#34;
        )
        return mul

    def transpose_kernel(self):
        transpose = SourceModule(
            &#34;&#34;&#34;
        #define BLOCK_SIZE %(block_size)d
        #define A_BLOCK_STRIDE (BLOCK_SIZE * a_width)
        #define A_T_BLOCK_STRIDE (BLOCK_SIZE * a_height)

        __global__ void transpose(float *A_t, float *A, int a_width, int a_height)
        {
            // Base indices in A and A_t
            int base_idx_a   = blockIdx.x * BLOCK_SIZE +
        blockIdx.y * A_BLOCK_STRIDE;
            int base_idx_a_t = blockIdx.y * BLOCK_SIZE +
        blockIdx.x * A_T_BLOCK_STRIDE;

            // Global indices in A and A_t
            int glob_idx_a   = base_idx_a + threadIdx.x + a_width * threadIdx.y;
            int glob_idx_a_t = base_idx_a_t + threadIdx.x + a_height * threadIdx.y;

            __shared__ float A_shared[BLOCK_SIZE][BLOCK_SIZE+1];

            // Store transposed submatrix to shared memory
            A_shared[threadIdx.y][threadIdx.x] = A[glob_idx_a];

            __syncthreads();

            // Write transposed submatrix to global memory
            A_t[glob_idx_a_t] = A_shared[threadIdx.x][threadIdx.y];
        }
        &#34;&#34;&#34;
        )
        return transpose

    def matrix_mul_kernel(self):
        matmul = &#34;&#34;&#34;
        __global__ void MatrixMulKernel(float *a, float *b, float *c)
        {
            int tx = threadIdx.x;
            int ty = threadIdx.y;

            float Pvalue = 0;

            for (int k = 0; k &lt; %(MATRIX_SIZE)s; ++k) {
                float Aelement = a[ty * %(MATRIX_SIZE)s + k];
                float Belement = b[k * %(MATRIX_SIZE)s + tx];
                Pvalue += Aelement * Belement;
            }

            c[ty * %(MATRIX_SIZE)s + tx] = Pvalue;
        }
        &#34;&#34;&#34;
        return matmul


kernel = Kernel()
add = kernel.addition_kernel()
mul = kernel.multiply_kernel()
transpose = None  # kernel.transpose_kernel()
arithmetic = kernel.arithmetic_kernel


ops = {&#34;+&#34;: operator.add, &#34;-&#34;: operator.sub, &#34;*&#34;: operator.mul, &#34;/&#34;: operator.truediv}


class GPUConnectMixin:
    &#34;&#34;&#34;Mixin for GPU connect&#34;&#34;&#34;

    def _alloc_device_memory(self, shape):
        &#34;&#34;&#34;_alloc_device_memory.
        Allocate memory  to device.

        Args:
            data:
        &#34;&#34;&#34;
        _nbytes = np.prod(shape) * 4
        _device_data = cuda.mem_alloc(int(_nbytes))
        _device_data.shape = tuple(shape)
        _device_data.dtype = np.float32
        return _device_data

    def _memory_host_to_device(self, device_data, data):
        &#34;&#34;&#34;_memory_host_to_device.
        Copy memory host to device(GPU).

        Args:
            data:
            device_data:
        &#34;&#34;&#34;
        cuda.memcpy_htod(device_data, data)
        return

    @staticmethod
    def _idiv(a, b):
        return a // b + 1

    @staticmethod
    def get_kernel(kernel, function):
        &#34;&#34;&#34;get_kernel.
        get the kernel.

        Args:
            kernel:
            function:
        &#34;&#34;&#34;
        return kernel.get_function(function)


class GradientMixin:
    &#34;&#34;&#34; Gradient Mixin class with grad tools. &#34;&#34;&#34;

    def _walk(self, leaf_out_node):
        &#34;&#34;&#34;_walk.
        Reverse Graph Traversal with gradients.

        Args:
            leaf_out_node: Leaf Node.
            in_grad: Input Gradient.
        &#34;&#34;&#34;
        self.visited.add(leaf_out_node)
        for node in leaf_out_node._child_nodes:
            if node not in self.visited:
                self._walk(node)
        self.nodes.append(leaf_out_node)
        return

    def backward(self):
        &#34;&#34;&#34;backward.
        Backward Function with Input Gradient set 1.

        Args:
            out_node: Leaf Output Node.
        &#34;&#34;&#34;
        self.visited = set()
        self.nodes = []
        self._walk(self)
        self.grad = 1.0
        for node in reversed(self.nodes):
            node._backward(node.grad)
        return self.grad


class Tensor(GPUConnectMixin, GradientMixin):
    &#34;&#34;&#34;Tensor Class.&#34;&#34;&#34;

    BLOCKSIZE = 256

    &#34;&#34;&#34;
    The dict wastes a lot of RAM. Python can’t just allocate a static amount of memory at
    object creation to store all the attributes. Therefore it sucks a lot of RAM if you
    create a lot of objects (I am talking in thousands and millions).
    Still there is a way to circumvent this issue.
    It involves the usage of __slots__ to tell Python not to use a dict,
    and only allocate space for a fixed set of attributes.
    &#34;&#34;&#34;

    __slots__ = (
        &#34;_data&#34;,
        &#34;_name&#34;,
        &#34;_n&#34;,
        &#34;_dtype&#34;,
        &#34;_shape&#34;,
        &#34;gpu&#34;,
        &#34;state&#34;,
        &#34;device_name&#34;,
    )

    def __init__(self, data, name=None, dtype=None):
        &#34;&#34;&#34;__init__.
        Initializes Tensor Class.

        Args:
            data: list or np.array data.
            gpu: use gpu?
        ::

        Example:

        &gt;&gt; a = Tensor([1, 2])
        &gt;&gt; b = Tensor([2,3])
        &gt;&gt; print(a + b)
        (dp.Tensor, shape=(2,), dtype = int32, numpy:([3,5], dtype = int32)
        &#34;&#34;&#34;
        self.state = TensorState.HOST
        if isinstance(data, (list, float, int)):
            data = np.array(data, dtype=dtype if dtype else np.float32)
        elif isinstance(data, pycuda._driver.DeviceAllocation):
            self.state = TensorState.DEVICE
        elif not (isinstance(data, np.ndarray) or isinstance(data, np.float32)):
            raise TypeError(f&#34;numpy excepted but {type(data)} passed.&#34;)
        self._data = data
        self._dtype = data.dtype
        self._shape = data.shape
        self._name = name
        self.gpu = False
        self.grad = 0.0
        self._child_nodes = tuple()

        def _backward(in_grad=0.0):
            self.grad = in_grad
            return (in_grad,)

        self._backward = _backward
        self.device_name = &#34;cpu:0&#34;

    def detach(self):
        &#34;&#34;&#34;detach.
        Detach state.
        &#34;&#34;&#34;
        self.state = TensorState.DETACH
        # TODO(kartik4949) : Write ME.
        return Tensor(self._data)

    @property
    def shape(self):
        return self._shape

    @property
    def name(self):
        return self._name

    @property
    def data(self):
        return self._data

    @property
    def dtype(self):
        return self._dtype

    @property
    def where(self):
        return self._device()

    def _device(self):
        if self.state == TensorState.DEVICE:
            _cuda_device = &#34;gpu&#34;

        if self.state == TensorState.HOST:
            _cuda_device = &#34;cpu&#34;
        return _cuda_device

    def asarray(self, data: list = None, dtype: tuple = None):
        &#34;&#34;&#34;asarray.
        convert array to DP array.

        Args:
            data (list): data
            dtype (tuple): dtype
        &#34;&#34;&#34;
        # Depracted!
        return Tensor(np.asarray(data, dtype=dtype))

    def device(self, name: str = None):
        &#34;&#34;&#34;device.
        register the data on device.

        Args:
            name (str): name of device
        &#34;&#34;&#34;
        assert name.startswith(&#34;cpu&#34;) or name.startswith(&#34;gpu&#34;), &#34;Wrong Device!!&#34;
        # set precision to float32.
        assert (
            self.dtype == np.float32
        ), &#34;Only single precision is supported i.e float32&#34;
        if self.state != TensorState.DEVICE:
            self.state = TensorState.DEVICE
            self.device_name = name
            data = self._alloc_device_memory(self.shape)
            self._memory_host_to_device(data, self._data)
            self._shape = self._data.shape
            self._dtype = self._data.dtype
            self._data = data
        return self

    def cpu(
        self,
    ):
        &#34;&#34;&#34;cpu.
        copy buffer from device to cpu.
        &#34;&#34;&#34;
        _host_out_arry = np.empty(self.shape, dtype=np.float32)
        cuda.memcpy_dtoh(_host_out_arry, self._data)
        cuda.Context.synchronize()
        return Tensor(_host_out_arry)

    def sigmoid(self):
        &#34;&#34;&#34;Sigmoid function.&#34;&#34;&#34;
        sig = 1 / (1 + np.exp(-self._data))
        ret = Tensor(sig)
        ret._child_nodes = (self,)

        def _backward(in_grad):
            self.grad += in_grad * (ret._data * (1 - ret._data))
            return self.grad

        ret._backward = _backward

        return ret

    def relu(self):
        &#34;&#34;&#34;Relu function.&#34;&#34;&#34;
        _data = np.maximum(self._data, 0)
        out = Tensor(_data)
        out._child_nodes = (self,)

        def _backward(in_grad):
            self.grad += (out._data &gt; 0) * in_grad
            return (self.grad,)

        out._backward = _backward
        return out

    def tanh(self):
        &#34;&#34;&#34;Tanh Function.&#34;&#34;&#34;
        t2 = Tensor(
            np.zeros(self.shape, dtype=self.data.dtype) + 2,
        )
        t1 = Tensor(np.zeros(self.shape, dtype=self.data.dtype))
        return self.mul(t2).sigmoid().mul(t2) - t1  # 2*sigmoid(2*x)-1

    #  def softmax(self):
    #      eX = np.exp((self - np.max(self)))
    #      return Tensor(eX/eX.sum(axis=0))

    def add(self, tensor):
        &#34;&#34;&#34;add.
        Vector Addition which adds Tensor with given Tensor.

        Args:
            tensor: Tensor class
        &#34;&#34;&#34;

        def _backward(in_grad):
            self.grad += in_grad
            tensor.grad += in_grad
            return in_grad, in_grad

        return self.arithmetic(tensor, _backward, &#34;+&#34;)

    def sub(self, tensor):
        &#34;&#34;&#34;sub.
        Vector Addition which substracts Tensor with given Tensor.

        Args:
            tensor: Tensor class
        &#34;&#34;&#34;

        def _backward(in_grad):
            self.grad += in_grad
            tensor.grad += -in_grad
            return in_grad, -in_grad

        return self.arithmetic(tensor, _backward, &#34;-&#34;)

    def mul(self, tensor):
        &#34;&#34;&#34;mul.
        Vector Addition which multiplies Tensor with given Tensor.

        Args:
            tensor: Tensor class
        &#34;&#34;&#34;

        def _backward(in_grad):
            self_grad = in_grad * tensor._data
            tensor_grad = in_grad * self._data
            self.grad += self_grad
            tensor.grad += tensor_grad
            return self_grad, tensor_grad

        return self.arithmetic(tensor, _backward, &#34;*&#34;)

    def arithmetic(self, tensor, backward=None, operation: str = &#34;+&#34;):
        &#34;&#34;&#34;Arithmetic.
        Vector arithmetic operations on given Tensor.

        Args:
            tensor: Tensor class
        &#34;&#34;&#34;
        if self.state != TensorState.DEVICE:
            ret = Tensor(ops[operation](self._data, tensor.data))
            ret._child_nodes = (self, tensor)
            if backward:
                ret._backward = backward
            return ret
        assert isinstance(
            tensor, self.__class__
        ), f&#34;Tensor is required but passed {type(tensor)}&#34;
        ret = self._alloc_device_memory(self.shape)
        N = max(self.shape)
        blockDim = (self.BLOCKSIZE, 1, 1)
        gridDim = (self._idiv(N, self.BLOCKSIZE), 1, 1)
        _vec_kernel = self.get_kernel(arithmetic(operation), &#34;device_arithmetic&#34;)
        _vec_kernel(
            ret,
            self._data,
            tensor.data,
            np.int32(N),
            block=blockDim,
            grid=gridDim,
        )

        ret = Tensor(ret)
        ret._dtype = self.dtype
        ret._shape = self.shape
        return ret

    def __pow__(self, value):
        out = Tensor(self.data ** value)
        out._child_nodes = (self,)

        def _backward(in_grad):
            self.grad += (value * self._data ** (value - 1)) * in_grad
            return (self.grad,)

        out._backward = _backward
        return out

    def __add__(self, tensor):
        tensor = tensor if isinstance(tensor, Tensor) else Tensor(tensor)
        return self.add(tensor)

    def __radd__(self, tensor):
        return self + tensor

    def __mul__(self, tensor):
        return self.mul(tensor)

    def __sub__(self, tensor):
        return self.sub(tensor)

    def __neg__(self):
        return self * -1

    def __rsub__(self, tensor):
        return tensor + (-self)

    def __rmul__(self, tensor):
        return self * tensor

    def __truediv__(self, value):
        return self * value ** -1

    def __rtruediv__(self, vale):
        return value * self ** -1

    def __repr__(self):
        return &#34;Tensor( %s shape: %s, numpy: (%s, dtype=%s), device: %s)&#34; % (
            f&#34;name: {self.name}, &#34; if self.name else &#34;&#34;,
            self.shape,
            self._data,
            self.dtype,
            self.where,
        )</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="tinydl" href="index.html">tinydl</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>